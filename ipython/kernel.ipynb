{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2c036df295691c7305bfd3d5cac5f225db157d4",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout ,BatchNormalization\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm_notebook,tnrange\n",
    "\n",
    "# 准备\n",
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]\n",
    "\n",
    "train_df = pd.read_csv(\"/home/zhangs/lyc/salt/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"/home/zhangs/lyc/salt/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]#将生成id不在train中的样本id集合\n",
    "train_df[\"images\"] = [np.array(load_img(\"/home/zhangs/lyc/salt/train/images/{}.png\".format(idx), grayscale=True))/ 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"/home/zhangs/lyc/salt/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "# 将深度信息放入训练图像\n",
    "MAX_DEPTH = max(train_df[\"z\"])\n",
    "print('**** Max depth in train set is :'+str(MAX_DEPTH))\n",
    "train_df[\"depth\"] = [np.ones_like(train_df.loc[i][\"images\"]) * train_df.loc[i][\"z\"] / MAX_DEPTH\n",
    "                     for i in tqdm_notebook(train_df.index)]\n",
    "\n",
    "# Image in layer1 + depth in layer2\n",
    "train_df[\"images_d\"] = [np.dstack((train_df[\"images\"][i],train_df[\"depth\"][i])) for i in tqdm_notebook(train_df.index)]\n",
    "train_df[\"images_d\"][0].shape\n",
    "# Free up some RAM\n",
    "del depths_df\n",
    "del train_df[\"images\"]\n",
    "# Sanity check\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangs/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNormalization in module keras.layers.normalization:\n",
      "\n",
      "class BatchNormalization(keras.engine.base_layer.Layer)\n",
      " |  Batch normalization layer (Ioffe and Szegedy, 2014).\n",
      " |  \n",
      " |  Normalize the activations of the previous layer at each batch,\n",
      " |  i.e. applies a transformation that maintains the mean activation\n",
      " |  close to 0 and the activation standard deviation close to 1.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      axis: Integer, the axis that should be normalized\n",
      " |          (typically the features axis).\n",
      " |          For instance, after a `Conv2D` layer with\n",
      " |          `data_format=\"channels_first\"`,\n",
      " |          set `axis=1` in `BatchNormalization`.\n",
      " |      momentum: Momentum for the moving mean and the moving variance.\n",
      " |      epsilon: Small float added to variance to avoid dividing by zero.\n",
      " |      center: If True, add offset of `beta` to normalized tensor.\n",
      " |          If False, `beta` is ignored.\n",
      " |      scale: If True, multiply by `gamma`.\n",
      " |          If False, `gamma` is not used.\n",
      " |          When the next layer is linear (also e.g. `nn.relu`),\n",
      " |          this can be disabled since the scaling\n",
      " |          will be done by the next layer.\n",
      " |      beta_initializer: Initializer for the beta weight.\n",
      " |      gamma_initializer: Initializer for the gamma weight.\n",
      " |      moving_mean_initializer: Initializer for the moving mean.\n",
      " |      moving_variance_initializer: Initializer for the moving variance.\n",
      " |      beta_regularizer: Optional regularizer for the beta weight.\n",
      " |      gamma_regularizer: Optional regularizer for the gamma weight.\n",
      " |      beta_constraint: Optional constraint for the beta weight.\n",
      " |      gamma_constraint: Optional constraint for the gamma weight.\n",
      " |  \n",
      " |  # Input shape\n",
      " |      Arbitrary. Use the keyword argument `input_shape`\n",
      " |      (tuple of integers, does not include the samples axis)\n",
      " |      when using this layer as the first layer in a model.\n",
      " |  \n",
      " |  # Output shape\n",
      " |      Same shape as input.\n",
      " |  \n",
      " |  # References\n",
      " |      - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNormalization\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  call(self, inputs, training=None)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout ,BatchNormalization\n",
    "\n",
    "help(BatchNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_train, ids_valid = train_test_split(\n",
    "    train_df.index.values,\n",
    "    test_size=0.05, stratify=train_df.coverage_class, random_state=1337)\n",
    "my_test_pd = train_df.loc[train_df.index.isin(ids_valid)]\n",
    "train_df = train_df.loc[train_df.index.isin(ids_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(my_test_pd.shape)\n",
    "print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut the train and valid set ，use K-flods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K_flods = 5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = train_df.index.values\n",
    "y = train_df.coverage_class\n",
    "skf = StratifiedKFold(n_splits=K_flods,random_state=1337)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)\n",
    "ids_train,ids_valid,x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test=[[] for x in range(10)]\n",
    "X_whole = np.array(train_df.images_d.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 2)\n",
    "y_whole = np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "for i,[train_index, test_index] in enumerate(skf.split(X, y)):\n",
    "    print(\"the %dth flod:\"%i)\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    ids_train.append(X[train_index])\n",
    "    ids_valid.append(X[test_index])\n",
    "    #\n",
    "    x_train.append(X_whole[train_index])\n",
    "    x_valid.append(X_whole[test_index])\n",
    "    #\n",
    "    y_train.append(y_whole[train_index])\n",
    "    y_valid.append(y_whole[test_index]) \n",
    "    #\n",
    "    cov_train.append(train_df.coverage.values[train_index]) \n",
    "    cov_test.append(train_df.coverage.values[test_index])\n",
    "    #\n",
    "    depth_train.append(train_df.z.values[train_index]) \n",
    "    depth_test.append(train_df.z.values[test_index]) \n",
    "del X_whole,y_whole\n",
    "print(len(x_train))\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee4da60a2585bfd1f87eb847b9737980c35a84ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 画各个flods的salt分布图，检验k-flods是否正确\n",
    "def plot_flods_coverage(cov,flods_num=5,mode='train'):\n",
    "    fig, axs = plt.subplots(1, flods_num+1, figsize=(15,5))\n",
    "    sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "    for i in range(1,flods_num+1):\n",
    "        sns.distplot(cov[i-1], bins=10, kde=False, ax=axs[i])\n",
    "        axs[i].set_xlabel(\"Coverage of k%d\"%(i-1))\n",
    "    plt.suptitle(\"Salt coverage of k-flods \"+mode)\n",
    "    axs[0].set_xlabel(\"Coverage\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_flods_coverage(cov_train,flods_num=5,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_flods_coverage(cov_test,flods_num=5,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57f2ca1eb4bea73b1126638dfdb1d756ef3c917a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sns.distplot(train_df.z, label=\"Train\")\n",
    "# sns.distplot(test_df.z, label=\"Test\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Depth distribution\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e527aa033723d7276e3ee35e7e90f14de9bbb043",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_images = 60\n",
    "# grid_width = 15\n",
    "# grid_height = int(max_images / grid_width)\n",
    "# fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "# for i, idx in enumerate(train_df.index[:max_images]):\n",
    "#     img = train_df.loc[idx].images\n",
    "#     mask = train_df.loc[idx].masks\n",
    "#     ax = axs[int(i / grid_width), i % grid_width]\n",
    "#     ax.imshow(img, cmap=\"Greys\")\n",
    "#     ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "#     ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "#     ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "#     ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "#     ax.set_yticklabels([])\n",
    "#     ax.set_xticklabels([])\n",
    "# plt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5927c77a3d6d9b5215b00dd1540a4d5e2ab501e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "#     train_df.index.values,\n",
    "#     np.array(train_df.images_d.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 2), \n",
    "#     np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "#     train_df.coverage.values,\n",
    "#     train_df.z.values,\n",
    "#     test_size=0.2, stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.util import pad\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "axs[0].imshow(x_train[0,:,:,0],  cmap=\"Greys\")\n",
    "axs[1].imshow(x_train[1,:,:,0],  cmap=\"Greys\")\n",
    " \n",
    "xpad1=pad(resize(x_train[0,:,:,0], (101*2, 101*2), mode='constant', preserve_range=True),27,'reflect')\n",
    "xpad2=pad(resize(x_train[1,:,:,0], (101*2, 101*2), mode='constant', preserve_range=True),27,'reflect')\n",
    "print(xpad1.shape)\n",
    "axs[2].imshow(xpad1,  cmap=\"Greys\")\n",
    "axs[3].imshow(xpad2,  cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36da20e77216c0824ab6cb4d6963603c6db736ed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\n",
    "# tmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\n",
    "# fix, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "# axs[0].imshow(tmp_img, cmap=\"Greys\")\n",
    "# axs[0].set_title(\"Original image\")\n",
    "# axs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\n",
    "# axs[1].set_title(\"Scaled image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "751db378b8b0ba53bf6bbd4da9ee405bc648bfe0"
   },
   "source": [
    "# Data argumantant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c28f33e17759ef77ce17423db1821d735f0507b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(K_flods):\n",
    "    x_l_r_flip = [np.fliplr(x) for x in x_train[i]]\n",
    "    y_l_r_flip = [np.fliplr(x) for x in y_train[i]]\n",
    "    # x_u_d_flip = [np.flipud(x) for x in x_train]\n",
    "    # y_u_d_flip = [np.flipud(x) for x in y_train]\n",
    "    # x_t = [np.reshape(np.transpose(x),(img_size_target,img_size_target,2)) for x in x_train]\n",
    "    # y_t = [np.reshape(np.transpose(x),(img_size_target,img_size_target,1)) for x in y_train]\n",
    "    # x_train = np.append(x_train, x_u_d_flip, axis=0)\n",
    "    # y_train = np.append(y_train, y_u_d_flip, axis=0)\n",
    "    # x_train = np.append(x_train, x_t, axis=0)\n",
    "    # y_train = np.append(y_train, y_t, axis=0)\n",
    "    x_train[i] = np.append(x_train[i], x_l_r_flip, axis=0)\n",
    "    y_train[i] = np.append(y_train[i], y_l_r_flip, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cac75b593bf29db252f09f4bd29e7ee938f6d00a",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "359a03c11c3ccf547f59709c5b00351f58ac6dc2"
   },
   "source": [
    "## 对比度增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "513d0ac24c783eb673a0c953a594475439628f70",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from skimage import exposure\n",
    "# contrast = [np.dstack([np.array(exposure.rescale_intensity(\n",
    "#     img[:,:,0],in_range=(np.percentile(img[:,:,0], (1, 99))[0],np.percentile(img[:,:,0], (1, 99))[1]))),img[:,:,1]]) \n",
    "#             for img in tqdm_notebook(x_train)]\n",
    "# contrast = np.array(contrast)\n",
    "# x_train = np.append(x_train, contrast, axis=0)\n",
    "# y_train = np.append(y_train, y_train, axis=0)\n",
    "# x_train = np.append(x_train, x_l_r_flip, axis=0)\n",
    "# y_train = np.append(y_train, y_l_r_flip, axis=0)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# del contrast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b5619ed63f9ca81965a0e05c94f6ea2ec59c3f8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x_train[110,:,:,0], cmap='binary')\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1,2,2)    \n",
    "plt.imshow(contrast[110,:,:,0], cmap='binary')\n",
    "plt.title('Contrast stretching')\n",
    "print(contrast.shape)\n",
    "print(contrast[110,60:64,60:64,1])\n",
    "print(x_train[110,60:64,60:64,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "793e717cbaa3e090c4e141eb4d9acac71551d745",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def random_shiff(x_train,y_train):\n",
    "    length = x_train.shape[0]\n",
    "    len_list = list(range(length))\n",
    "    shiff_index = np.random.shuffle(len_list)\n",
    "    return x_train[shiff_index:,:,:],y_train[shiff_index:,:,:]\n",
    "x_train,y_train = random_shiff(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d1550ba131ae8af33bedb2a0d1eee7bf420f6ed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(np.array(x_t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9450f13a7c48befebc835a60a67356c4cb715b2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fix, axs = plt.subplots(2, 2, figsize=(15,5))\n",
    "print(x_train.shape)\n",
    "axs[0][0].imshow(x_train[10,:,:,0], cmap=\"Greys\")\n",
    "axs[0][1].imshow(y_train[10,:,:,0], cmap=\"Greys\")\n",
    "axs[1][0].imshow((np.fliplr(x_train[10,:,:,0])), cmap=\"Greys\")\n",
    "axs[1][1].imshow((np.fliplr(y_train[10,:,:,0])), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73a311a8fa1ae5ebdc2d5fc7031878da65c17a7f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image, ImageEnhance, ImageOps, ImageFile\n",
    "# def randomRotation(image, mode=Image.BICUBIC):\n",
    "#     \"\"\"\n",
    "#      对图像进行随机任意角度(0~360度)旋转\n",
    "#     :param mode 邻近插值,双线性插值,双三次B样条插值(default)\n",
    "#     :param image PIL的图像image\n",
    "#     :return: 旋转转之后的图像\n",
    "#     \"\"\"\n",
    "#     random_angle = np.random.randint(1, 360)\n",
    "#     return image.rotate(random_angle, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b25e05e023add8abee16ba7303b91822fd2a017",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
    "# for i in range(10):\n",
    "#     axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n",
    "#     axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "#     axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "#     axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "# fig.suptitle(\"Top row: original images, bottom row: augmented images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[0][..., :1].shape\n",
    "x_train[0][:, :DPT_SIZE, :DPT_SIZE, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_iou(Y_true, Y_pred, score_thres=0.5):\n",
    "    \"\"\"Compute mean(IoU) metric\n",
    "    IoU = intersection / union\n",
    "    \n",
    "    For each (mask)threshold in provided range:\n",
    "     - convert probability mask to boolean mask based on given threshold\n",
    "     - score the mask 1 if(IoU > score_threshold(0.5))\n",
    "    Take the mean of the scoress\n",
    "\n",
    "    https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou\n",
    "    \"\"\"\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        Y_pred_bool = tf.to_int32(Y_pred > t) # boolean mask by threshold\n",
    "        score, update_op = tf.metrics.mean_iou(Y_true, Y_pred_bool, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            score = tf.identity(score) #!! use identity to transform score to tensor\n",
    "        prec.append(score) \n",
    "        \n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "\n",
    "def conv_block(m, ch_dim, acti='relu', bn=True, res=True, drop=0):\n",
    "    \"\"\"CNN block\"\"\"\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(drop)(n) if drop else n\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return concatenate([m, n]) if res else n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpt = 5\n",
    "DPT_SIZE = int(img_size_target/pow(2,dpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(start_neurons):\n",
    "    inputs = Input(shape=(img_size_target, img_size_target, 1), name='img')\n",
    "    input_depth = Input(shape=(DPT_SIZE, DPT_SIZE, 1), name='depth')\n",
    "    # 128 -> 64\n",
    "    conv1 = conv_block(inputs, start_neurons*1, acti='relu', bn=True, res=True, drop=0)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    # 64 -> 32\n",
    "    conv2 = conv_block(pool1, start_neurons*2, acti='relu', bn=True, res=True, drop=0)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    # 32 -> 16\n",
    "    conv3 = conv_block(pool2, start_neurons*4, acti='relu', bn=True, res=True, drop=0)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "\n",
    "    # 16 -> 8\n",
    "    conv4 = conv_block(pool3, start_neurons*8, acti='relu', bn=True, res=True, drop=0)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    \n",
    "    # 8 -> 4\n",
    "    conv5 = conv_block(pool4, start_neurons*16, acti='relu', bn=True, res=True, drop=0)\n",
    "    pool5 = MaxPooling2D((2, 2))(conv5)\n",
    "    \n",
    "    # Middle\n",
    "    convm = conv_block(pool5, start_neurons*32, acti='relu', bn=True, res=True, drop=0.5)\n",
    "    convm = concatenate([convm, input_depth])\n",
    "    \n",
    "    # 4 -> 8\n",
    "    deconv5 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv5 = concatenate([deconv5, conv5])\n",
    "    uconv5 = conv_block(uconv5, start_neurons * 16, acti='relu', bn=True, res=True, drop=0)\n",
    "    \n",
    "    # 8 -> 16\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv5)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = conv_block(uconv4, start_neurons*8, acti='relu', bn=True, res=True, drop=0)\n",
    "\n",
    "    # 16 -> 32\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = conv_block(uconv3, start_neurons*4, acti='relu', bn=True, res=True, drop=0)\n",
    "\n",
    "    # 32 -> 64\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = conv_block(uconv2, start_neurons*2, acti='relu', bn=True, res=True, drop=0)\n",
    "\n",
    "    # 64 -> 128\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = conv_block(uconv1, start_neurons*1, acti='relu', bn=True, res=True, drop=0)\n",
    "\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "    return Model(inputs=[inputs,input_depth], outputs=output_layer)\n",
    "\n",
    "# output_layer = build_model(input_layer[:,:,:,:1], 16,input_layer[:,:8,:8,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "5c4e9830d4141afbc33423df276ec2051da8e224",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_all = []\n",
    "fig, axs = plt.subplots(K_flods, 3, figsize=(15,5))\n",
    "for i in range(K_flods): \n",
    "    model = build_model(start_neurons=32)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",mean_iou])\n",
    "#     model.summary()\n",
    "    early_stopping = EarlyStopping(patience=6, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(\"trained_models/%dth_flod.model\"%i, save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    epochs = 200\n",
    "    batch_size = 32\n",
    "\n",
    "    history = model.fit({'img': x_train[i][..., :1], \n",
    "                         'depth': x_train[i][:, 60:60+DPT_SIZE, 60:60+DPT_SIZE, 1:]},\n",
    "                        y_train[i],\n",
    "                        validation_data=({'img': x_valid[i][..., :1], \n",
    "                                          'depth': x_valid[i][:, 60:60+DPT_SIZE, 60:60+DPT_SIZE, 1:]},\n",
    "                        y_valid[i]), \n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "    history_all.append(history)\n",
    "    axs[i][0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    axs[i][0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    axs[i][1].plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "    axs[i][1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "    axs[i][2].plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
    "    axs[i][2].plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predit with K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predit_with_kfolds(K_flods,x_img,x_depth):\n",
    "    preds_valid_all = []\n",
    "    for i in range(K_flods):\n",
    "        model_flods = load_model(\"trained_models/%dth_flod.model\"%i, custom_objects={'mean_iou': mean_iou})\n",
    "    #     model.append(model_flods)\n",
    "        #此处的validation为第0组flod\n",
    "        preds_valid_flods = model_flods.predict({'img': x_img, \n",
    "                                'depth': x_depth}).reshape(-1, img_size_target, img_size_target)\n",
    "        preds_valid_flods = np.array([downsample(x) for x in preds_valid_flods])\n",
    "        preds_valid_all.append(preds_valid_flods)\n",
    "    preds_valid = (preds_valid_all[0]+preds_valid_all[1]+preds_valid_all[2]+preds_valid_all[3]+preds_valid_all[4])/5\n",
    "    return preds_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53ff83d85555e9e1d94c07c49396a5e0230a9cd0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_x = np.array(my_test_pd.images_d.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 2)\n",
    "valid_y = np.array(my_test_pd.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "\n",
    "preds_valid = predit_with_kfolds(K_flods,valid_x[..., :1],valid_x[:, 60:60+DPT_SIZE, 60:60+DPT_SIZE, 1:])\n",
    "# preds_valid_all = []\n",
    "# for i in range(K_flods):\n",
    "#     model_flods = load_model(\"trained_models/%dth_flod.model\"%i, custom_objects={'mean_iou': mean_iou})\n",
    "# #     model.append(model_flods)\n",
    "#     #此处的validation为第0组flod\n",
    "#     preds_valid_flods = model_flods.predict({'img': x_valid[0][..., :1], \n",
    "#                             'depth': x_valid[0][:, 60:60+DPT_SIZE, 60:60+DPT_SIZE, 1:]}).reshape(-1, img_size_target, img_size_target)\n",
    "#     preds_valid_flods = np.array([downsample(x) for x in preds_valid_flods])\n",
    "#     preds_valid_all.append(preds_valid_flods)\n",
    "# preds_valid = (preds_valid_all[0]+preds_valid_all[1]+preds_valid_all[2]+preds_valid_all[3]+preds_valid_all[4])/5\n",
    "y_valid = np.array([downsample(x) for x in valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e13865d3cb826f3c0d45c9146342e0253d0a8e77",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_images = 60\n",
    "# grid_width = 15\n",
    "# grid_height = int(max_images / grid_width)\n",
    "# fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "# for i, idx in enumerate(ids_valid[60:60+max_images]):\n",
    "#     img = train_df.loc[idx].images\n",
    "#     mask = train_df.loc[idx].masks\n",
    "#     pred = preds_valid[i]\n",
    "#     ax = axs[int(i / grid_width), i % grid_width]\n",
    "#     ax.imshow(img, cmap=\"Greys\")\n",
    "#     ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "#     ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "#     ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "#     ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "#     ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "#     ax.set_yticklabels([])\n",
    "#     ax.set_xticklabels([])\n",
    "# plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5045277ad87d64a3ed3f43eff7862ef19131177c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a85541b44a6b65a8616e26e0950f2ba1b7011bd0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(y_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])\n",
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad0d1079cf033cd6bd472cbe7d1ad8d6bb852293",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd41507b1c26a16f47abf48cad2b6ab81be5508c"
   },
   "source": [
    "\n",
    "# Another sanity check with adjusted threshold\n",
    "\n",
    "Again some sample images with the adjusted threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "212e2c49e7ce3b065c456ee5a9f6ad103744f511",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_images = 60\n",
    "# grid_width = 15\n",
    "# grid_height = int(max_images / grid_width)\n",
    "# fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "# for i, idx in enumerate(ids_valid[:max_images]):\n",
    "#     img = train_df.loc[idx].images\n",
    "#     mask = train_df.loc[idx].masks\n",
    "#     pred = preds_valid[i]\n",
    "#     ax = axs[int(i / grid_width), i % grid_width]\n",
    "#     ax.imshow(img, cmap=\"Greys\")\n",
    "#     ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "#     ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n",
    "#     ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "#     ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "#     ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "#     ax.set_yticklabels([])\n",
    "#     ax.set_xticklabels([])\n",
    "# plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe077821181c226c4cfc2ea1c0844314cb61a182"
   },
   "source": [
    "\n",
    "# Submission\n",
    "\n",
    "Load, predict and submit the test image predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e650eb0e1773f9de1e3c555b1dc1961dbb30a8f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6be1a40a948e5bad6e165d2228a613ed16534224",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([upsample(np.array(load_img(\"test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "# Create depth layer\n",
    "del x_train,y_train,x_valid,y_valid\n",
    "x_test_d = [np.ones((DPT_SIZE,DPT_SIZE,1)) * (test_df.loc[i][\"z\"] / MAX_DEPTH)\n",
    "                     for i in tqdm_notebook(test_df.index)] \n",
    "x_test_d = np.array(x_test_d).reshape(-1, DPT_SIZE, DPT_SIZE, 1)\n",
    "x_test_d.shape\n",
    "\n",
    "preds_test = predit_with_kfolds(K_flods,x_test,x_test_d)\n",
    "# preds_test = model.predict({'img': x_test, 'depth': x_test_d})\n",
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > \n",
    "            threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\n",
    "\n",
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
