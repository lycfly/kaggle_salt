{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2c036df295691c7305bfd3d5cac5f225db157d4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout ,BatchNormalization\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm_notebook,tnrange\n",
    "from skimage.util import pad\n",
    "\n",
    "# 准备\n",
    "img_size_ori = 101\n",
    "img_size_target = 256\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "def upsample_v2(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (256, 256), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res    \n",
    "def reflect_pad(img):\n",
    "    return pad(resize(img, (101*2, 101*2), mode='constant', preserve_range=True),27,'reflect')\n",
    "\n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]\n",
    "\n",
    "train_df = pd.read_csv(\"/home/zhangs/lyc/salt/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"/home/zhangs/lyc/salt/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]#将生成id不在train中的样本id集合\n",
    "train_df[\"images\"] = [np.array(load_img(\"/home/zhangs/lyc/salt/train/images/{}.png\".format(idx), grayscale=True))/ 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"/home/zhangs/lyc/salt/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "# 将深度信息放入训练图像\n",
    "MAX_DEPTH = max(train_df[\"z\"])\n",
    "print('**** Max depth in train set is :'+str(MAX_DEPTH))\n",
    "train_df[\"depth\"] = [np.ones_like(train_df.loc[i][\"images\"]) * train_df.loc[i][\"z\"] / MAX_DEPTH\n",
    "                     for i in tqdm_notebook(train_df.index)]\n",
    "\n",
    "# Image in layer1 + depth in layer2\n",
    "train_df[\"images_d\"] = [np.dstack((train_df[\"images\"][i],train_df[\"depth\"][i])) for i in tqdm_notebook(train_df.index)]\n",
    "train_df[\"images_d\"][0].shape\n",
    "# Free up some RAM\n",
    "del depths_df\n",
    "# del train_df[\"images\"]\n",
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut the train and valid set ，use K-flods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K_flods = 5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = train_df.index.values\n",
    "y = train_df.coverage_class\n",
    "skf = StratifiedKFold(n_splits=K_flods,random_state=1337)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)\n",
    "ids_train,ids_valid,x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test=[[] for x in range(10)]\n",
    "X_whole = np.array(train_df.images.map(reflect_pad).tolist()).reshape(-1, 256, 256, 1)\n",
    "y_whole = np.array(train_df.masks.map(reflect_pad).tolist()).reshape(-1, 256, 256, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,[train_index, test_index] in enumerate(skf.split(X, y)):\n",
    "    print(\"the %dth flod:\"%i)\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    ids_train.append(X[train_index])\n",
    "    ids_valid.append(X[test_index])\n",
    "    #\n",
    "    x_train.append(X_whole[train_index])\n",
    "    x_valid.append(X_whole[test_index])\n",
    "    #\n",
    "    y_train.append(y_whole[train_index])\n",
    "    y_valid.append(y_whole[test_index]) \n",
    "    #\n",
    "    cov_train.append(train_df.coverage.values[train_index]) \n",
    "    cov_test.append(train_df.coverage.values[test_index])\n",
    "    #\n",
    "    depth_train.append(train_df.z.values[train_index]) \n",
    "    depth_test.append(train_df.z.values[test_index]) \n",
    "    \n",
    "    if i == 1:\n",
    "        break\n",
    "del X_whole,y_whole\n",
    "print(len(x_train))\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee4da60a2585bfd1f87eb847b9737980c35a84ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 画各个flods的salt分布图，检验k-flods是否正确\n",
    "def plot_flods_coverage(cov,flods_num=5,mode='train'):\n",
    "    fig, axs = plt.subplots(1, flods_num+1, figsize=(15,5))\n",
    "    sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "    for i in range(1,flods_num+1):\n",
    "        sns.distplot(cov[i-1], bins=10, kde=False, ax=axs[i])\n",
    "        axs[i].set_xlabel(\"Coverage of k%d\"%(i-1))\n",
    "    plt.suptitle(\"Salt coverage of k-flods \"+mode)\n",
    "    axs[0].set_xlabel(\"Coverage\")\n",
    "    plt.show()\n",
    "plot_flods_coverage(cov_train,flods_num=5,mode='train') \n",
    "plot_flods_coverage(cov_test,flods_num=5,mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "751db378b8b0ba53bf6bbd4da9ee405bc648bfe0"
   },
   "source": [
    "# Data argumantant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c28f33e17759ef77ce17423db1821d735f0507b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    x_l_r_flip = [np.fliplr(x) for x in x_train[i]]\n",
    "    y_l_r_flip = [np.fliplr(x) for x in y_train[i]]\n",
    "\n",
    "    x_train[i] = np.append(x_train[i], x_l_r_flip, axis=0)\n",
    "    y_train[i] = np.append(y_train[i], y_l_r_flip, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cac75b593bf29db252f09f4bd29e7ee938f6d00a",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (loss.py, line 70)",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/zhangs/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2862\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e39693a1f495>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pipline.loss import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/zhangs/lyc/salt/pipline/loss.py\"\u001b[0;36m, line \u001b[0;32m70\u001b[0m\n\u001b[0;31m    from __future__ import print_function, division\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from pipline.loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpt = 5\n",
    "DPT_SIZE = int(img_size_target/pow(2,dpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from segmentation_models.segmentation_models import Unet\n",
    "from segmentation_models.segmentation_models.utils import set_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "5c4e9830d4141afbc33423df276ec2051da8e224",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_all = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "for i in range(0,1): \n",
    "    model = Unet(input_shape=(256,256,3),backbone_name='resnet34', encoder_weights='imagenet', freeze_encoder=False,decoder_use_batchnorm=True)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\",mean_iou])\n",
    "\n",
    "    model.summary()\n",
    "    # continue training    \"trained_models/%dth_flod.model\"%i\n",
    "    early_stopping = EarlyStopping(monitor = 'mean_iou',mode='max',patience=5, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(\"trained_models/%dth_flod.model\"%i, save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',mode='min',factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    reduce_lr0 = ReduceLROnPlateau(monitor = 'val_loss',mode='min',factor=0.5, patience=1, min_lr=0.00001, verbose=1)\n",
    "    #pretrain model decoder\n",
    "#     model.fit(np.repeat(x_train[i][..., :1],3,axis=-1),\n",
    "#               y_train[i],\n",
    "#               validation_data=(np.repeat(x_valid[i][..., :1],3,axis=-1),\n",
    "#               y_valid[i]), \n",
    "#               epochs=4,\n",
    "#               batch_size=32,\n",
    "#               callbacks=[model_checkpoint, reduce_lr0])\n",
    "#     # release all layers for training\n",
    "#     set_trainable(model) # set all layers trainable and recompile model\n",
    "    \n",
    "    #model.compile(loss=focal_loss, optimizer=\"sgd\", metrics=[\"accuracy\",mean_iou])\n",
    "    history = model.fit(np.repeat(x_train[i][..., :1],3,axis=-1),\n",
    "                          y_train[i],\n",
    "                          validation_data=(np.repeat(x_valid[i][..., :1],3,axis=-1),\n",
    "                          y_valid[i]), \n",
    "                          epochs=160,\n",
    "                          batch_size=32,\n",
    "                          callbacks=[model_checkpoint, reduce_lr,early_stopping])\n",
    "    history_all.append(history)\n",
    "    axs[i][0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    axs[i][0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    axs[i][1].plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "    axs[i][1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "    axs[i][2].plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
    "    axs[i][2].plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predit with K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predit_with_kfolds(K_flods,x_img):\n",
    "    preds_valid_all = []\n",
    "    for i in range(K_flods):\n",
    "        model_flods = load_model(\"trained_models/%dth_flod.model\"%i, custom_objects={'mean_iou': mean_iou})\n",
    "    #     model.append(model_flods)\n",
    "        #此处的validation为第0组flod\n",
    "        preds_valid_flods = model_flods.predict(np.repeat(x_img,3,axis=-1))\n",
    "        print(preds_valid_flods.shape)\n",
    "        preds_valid_flods = np.array([downsample(x) for x in preds_valid_flods[:,27:229,27:229,:]])\n",
    "        print(preds_valid_flods.shape)\n",
    "        preds_valid_all.append(preds_valid_flods)\n",
    "    preds_valid = (preds_valid_all[0]+preds_valid_all[1]+preds_valid_all[2]+preds_valid_all[3]+preds_valid_all[4])/5\n",
    "    return preds_valid\n",
    "\n",
    "def predit_with_one_fold(model_num,x_img):\n",
    "    model_flods = load_model(\"trained_models/%dth_flod.model\"%model_num, custom_objects={'mean_iou': mean_iou})\n",
    "    #     model.append(model_flods)\n",
    "        #此处的validation为第0组flod\n",
    "    preds_valid_flods = model_flods.predict(np.repeat(x_img,3,axis=-1))\n",
    "    print(preds_valid_flods.shape)\n",
    "    preds_valid_flods = np.array([downsample(x) for x in preds_valid_flods[:,27:229,27:229,:]])\n",
    "    print(preds_valid_flods.shape)\n",
    "    return preds_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_x = np.array(my_test_pd.images.map(reflect_pad).tolist()).reshape(-1, 256, 256, 1)\n",
    "valid_y = np.array(my_test_pd.masks.map(reflect_pad).tolist()).reshape(-1, 256, 256, 1)\n",
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one model predit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid = predit_with_one_fold(0,valid_x)\n",
    "y_valid = np.array([downsample(x) for x in valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_flods = load_model(\"finetue0\", custom_objects={'mean_iou': mean_iou})\n",
    "preds_valid_flods = model_flods.predict(np.repeat(x_valid[0][..., :1],3,axis=-1))\n",
    "print(preds_valid_flods.shape)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid_flods[:,27:229,27:229,:]])\n",
    "print(preds_valid_flods.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_valid = np.array([downsample(x) for x in  y_valid[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53ff83d85555e9e1d94c07c49396a5e0230a9cd0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid = predit_with_kfolds(K_flods,valid_x)\n",
    "# preds_valid_all = []\n",
    "# for i in range(K_flods):\n",
    "#     model_flods = load_model(\"trained_models/%dth_flod.model\"%i, custom_objects={'mean_iou': mean_iou})\n",
    "# #     model.append(model_flods)\n",
    "#     #此处的validation为第0组flod\n",
    "#     preds_valid_flods = model_flods.predict({'img': x_valid[0][..., :1], \n",
    "#                             'depth': x_valid[0][:, 60:60+DPT_SIZE, 60:60+DPT_SIZE, 1:]}).reshape(-1, img_size_target, img_size_target)\n",
    "#     preds_valid_flods = np.array([downsample(x) for x in preds_valid_flods])\n",
    "#     preds_valid_all.append(preds_valid_flods)\n",
    "# preds_valid = (preds_valid_all[0]+preds_valid_all[1]+preds_valid_all[2]+preds_valid_all[3]+preds_valid_all[4])/5\n",
    "y_valid = np.array([downsample(x) for x in valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e13865d3cb826f3c0d45c9146342e0253d0a8e77",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_images = 60\n",
    "# grid_width = 15\n",
    "# grid_height = int(max_images / grid_width)\n",
    "# fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "# for i, idx in enumerate(ids_valid[60:60+max_images]):\n",
    "#     img = train_df.loc[idx].images\n",
    "#     mask = train_df.loc[idx].masks\n",
    "#     pred = preds_valid[i]\n",
    "#     ax = axs[int(i / grid_width), i % grid_width]\n",
    "#     ax.imshow(img, cmap=\"Greys\")\n",
    "#     ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "#     ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "#     ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "#     ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "#     ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "#     ax.set_yticklabels([])\n",
    "#     ax.set_xticklabels([])\n",
    "# plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5045277ad87d64a3ed3f43eff7862ef19131177c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a85541b44a6b65a8616e26e0950f2ba1b7011bd0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(y_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])\n",
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad0d1079cf033cd6bd472cbe7d1ad8d6bb852293",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd41507b1c26a16f47abf48cad2b6ab81be5508c"
   },
   "source": [
    "\n",
    "# Another sanity check with adjusted threshold\n",
    "\n",
    "Again some sample images with the adjusted threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "212e2c49e7ce3b065c456ee5a9f6ad103744f511",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_images = 60\n",
    "# grid_width = 15\n",
    "# grid_height = int(max_images / grid_width)\n",
    "# fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "# for i, idx in enumerate(ids_valid[:max_images]):\n",
    "#     img = train_df.loc[idx].images\n",
    "#     mask = train_df.loc[idx].masks\n",
    "#     pred = preds_valid[i]\n",
    "#     ax = axs[int(i / grid_width), i % grid_width]\n",
    "#     ax.imshow(img, cmap=\"Greys\")\n",
    "#     ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "#     ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n",
    "#     ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "#     ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "#     ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "#     ax.set_yticklabels([])\n",
    "#     ax.set_xticklabels([])\n",
    "# plt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe077821181c226c4cfc2ea1c0844314cb61a182"
   },
   "source": [
    "\n",
    "# Submission\n",
    "\n",
    "Load, predict and submit the test image predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e650eb0e1773f9de1e3c555b1dc1961dbb30a8f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6be1a40a948e5bad6e165d2228a613ed16534224",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([reflect_pad(np.array(load_img(\"test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "# Create depth layer\n",
    "del x_train,y_train,x_valid,y_valid\n",
    "# x_test_d = [np.ones((DPT_SIZE,DPT_SIZE,1)) * (test_df.loc[i][\"z\"] / MAX_DEPTH)\n",
    "#                      for i in tqdm_notebook(test_df.index)] \n",
    "# x_test_d = np.array(x_test_d).reshape(-1, DPT_SIZE, DPT_SIZE, 1)\n",
    "# x_test_d.shape\n",
    "preds_test = predit_with_one_fold(0,x_test)[:,27:229,27:229,:]\n",
    "\n",
    "# preds_test = predit_with_kfolds(K_flods,x_test)[:,27:229,27:229,:]\n",
    "# preds_test = model.predict({'img': x_test, 'depth': x_test_d})\n",
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > \n",
    "            threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\n",
    "\n",
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
